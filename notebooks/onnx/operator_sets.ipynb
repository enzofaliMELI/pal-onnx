{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Zd8YhHCgmaE5"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "from onnx import TensorProto, save, load\n",
        "from onnx.helper import (\n",
        "    make_model, make_node, make_tensor, make_graph,\n",
        "    make_tensor_value_info, make_opsetid)\n",
        "from onnx.checker import check_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3baYrCrKwfe"
      },
      "source": [
        "# StringSplit\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#StringSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Fnl6l6vpfgAL",
        "outputId": "b4b18dbb-1618-41e1-bf9d-8bd663372c91"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "string_input = make_tensor_value_info('string_input', TensorProto.STRING, [None])\n",
        "\n",
        "# Intermediate tensor after splitting\n",
        "string_split = make_tensor_value_info('string_split', TensorProto.STRING, [None, None])\n",
        "\n",
        "# StringSplit node\n",
        "split_node = make_node(\n",
        "    'StringSplit',\n",
        "    inputs=['string_input'],\n",
        "    outputs=['string_split', 'unused1'],\n",
        "    delimiter=\"\",\n",
        "    maxsplit=None,\n",
        ")\n",
        "\n",
        "# Create the graph with the splitting node\n",
        "split_graph = make_graph(\n",
        "    [split_node], \n",
        "    'splitter_model', \n",
        "    [string_input], \n",
        "    [string_split]\n",
        "    )\n",
        "\n",
        "# Specify opset versions\n",
        "split_model = make_model(\n",
        "    split_graph,\n",
        "    opset_imports=[\n",
        "        make_opsetid('', 20)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(split_model)\n",
        "\n",
        "# Save the split model to a file\n",
        "with open(\"split_node.onnx\", \"wb\") as f:\n",
        "    f.write(split_model.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxIkBY1Ifiqp"
      },
      "outputs": [],
      "source": [
        "ort_sess = ort.InferenceSession('split_node.onnx')\n",
        "\n",
        "# x = [\"NOT not do SOME the oov\"]\n",
        "# x = [\"I love computer science !\"]\n",
        "input_strings = np.array([\"Hello World\", \"I love computer science !\"])\n",
        "\n",
        "outputs = ort_sess.run(None, {'string_input': input_strings})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPoIJgFevuCh"
      },
      "source": [
        "# Tokenizer\n",
        "https://github.com/microsoft/onnxruntime/blob/main/docs/ContribOperators.md#commicrosofttokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6tNBLGlBwTI8"
      },
      "outputs": [],
      "source": [
        "# Define the Tokenizer node\n",
        "node = make_node(\n",
        "    op_type=\"Tokenizer\",\n",
        "    inputs=[\"x\"],\n",
        "    outputs=[\"y\"],\n",
        "    mark=0,  # Mark the beginning/end character\n",
        "    mincharnum=1,  # Minimum number of characters allowed\n",
        "    pad_value=\"\",  # Padding value\n",
        "    separators=[\" \"],  # List of separators (space)\n",
        "    domain=\"com.microsoft\"  # Specify the domain\n",
        ")\n",
        "\n",
        "# Create ONNX graph\n",
        "tokenizer_graph = make_graph(\n",
        "    [node],\n",
        "    \"tokenizer_graph\",\n",
        "    inputs=[\n",
        "        make_tensor_value_info(\"x\", TensorProto.STRING, [None])  # Input shape [None], allowing variable-length input\n",
        "    ],\n",
        "    outputs=[\n",
        "        make_tensor_value_info(\"y\", TensorProto.STRING, [1, None])  # Output shape [None, None]\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create ONNX model\n",
        "tokenizer_node = make_model(\n",
        "    tokenizer_graph,\n",
        "    opset_imports=[\n",
        "        make_opsetid('com.microsoft', 1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Verify the model\n",
        "check_model(tokenizer_node)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"tokenizer_node.onnx\", \"wb\") as f:\n",
        "    f.write(tokenizer_node.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YS3_yAd0TA6",
        "outputId": "46be9778-4c1a-4de1-e28c-5a2f3daea075"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Hello' 'World']]\n"
          ]
        }
      ],
      "source": [
        "# Load the ONNX model\n",
        "sess = ort.InferenceSession(\"tokenizer_node.onnx\")\n",
        "\n",
        "# Define the input data\n",
        "input_strings = np.array([\"Hello World\"])\n",
        "\n",
        "# Perform inference\n",
        "output = sess.run(None, {\"x\": input_strings.astype(object)})\n",
        "\n",
        "# Print the tokenized output\n",
        "print(output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxqcUiE3K5OW"
      },
      "source": [
        "# Normalizer\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#StringNormalizer\n",
        "No se puede hacer en batch dims [C] o [1, C]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DTlJt-5FGXJu"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "string_input = make_tensor_value_info('string_input', TensorProto.STRING, [1, None])\n",
        "\n",
        "# Output tensor\n",
        "string_normalized = make_tensor_value_info('string_normalized', TensorProto.STRING, [None, None])\n",
        "\n",
        "# Lowercasing node\n",
        "normalizer_node = make_node(\n",
        "    \"StringNormalizer\",\n",
        "    inputs=[\"string_input\"],\n",
        "    outputs=[\"string_normalized\"],\n",
        "    case_change_action=\"LOWER\",\n",
        "    stopwords=[\"the\"]\n",
        ")\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "normalizer_graph = make_graph([normalizer_node],\n",
        "                   'normalizer_model',\n",
        "                   [string_input],\n",
        "                   [string_normalized])\n",
        "\n",
        "# Specify opset versions\n",
        "normalizer_model = make_model(\n",
        "    normalizer_graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('', 10)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(normalizer_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"normalizer_node.onnx\", \"wb\") as f:\n",
        "    f.write(normalizer_model.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLdKSNVlGYSv",
        "outputId": "a67fb490-188c-4cd4-ee64-63288940b92a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([['not', 'not', 'do', 'some', 'oov']], dtype=object)]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('normalizer_node.onnx')\n",
        "x = [['NOT', 'not', 'do', 'SOME', 'the', 'oov']]\n",
        "outputs = ort_sess.run(None, {'string_input': x})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTL-S8MOQY3"
      },
      "source": [
        "# Mapper\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#ai.onnx.ml.CategoryMapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "XwPOVIXWOfp8"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "string_input = make_tensor_value_info('string_input', TensorProto.STRING, [1, None])\n",
        "\n",
        "# Output tensor\n",
        "numeric_output = make_tensor_value_info('numeric_output', TensorProto.INT64, [1, None])\n",
        "\n",
        "# CategoryMapper node\n",
        "mapper_node = make_node(\n",
        "    'CategoryMapper',\n",
        "    ['string_input'],\n",
        "    ['numeric_output'],\n",
        "    cats_strings=[\"do\", \"not\"],  # Vocabulary for mapping\n",
        "    cats_int64s=[1, 2],  # Integer mapping of\n",
        "    default_int64=0, # Default oov token\n",
        "    domain='ai.onnx.ml'\n",
        ")\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "mapper_graph = make_graph([mapper_node],\n",
        "                   'mapper_model',\n",
        "                   [string_input],\n",
        "                   [numeric_output])\n",
        "\n",
        "# Specify opset versions\n",
        "mapper_model = make_model(\n",
        "    mapper_graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('ai.onnx.ml', 1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(mapper_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"mapper_node.onnx\", \"wb\") as f:\n",
        "    f.write(mapper_model.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2nxUtQWOhPw",
        "outputId": "811f9d74-a57f-4fcf-e678-8ce082ffe58f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[2, 2, 1, 0, 0]], dtype=int64)]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('mapper_node.onnx')\n",
        "x = [['not', 'not', 'do', 'some', 'oov']]\n",
        "outputs = ort_sess.run(None, {'string_input': x})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TQKxE65u6XT"
      },
      "source": [
        "# Squeeze\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#Squeeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "klqVSpIMtF9s"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "input_tensor = make_tensor_value_info('input_tensor', TensorProto.INT64, [1, None])\n",
        "\n",
        "# Shape tensor to specify the output shape\n",
        "squeezed_tensor = make_tensor_value_info('squeezed_tensor', TensorProto.INT64, [None])\n",
        "\n",
        "# Create Squeeze node\n",
        "squeeze_node = make_node(\n",
        "    \"Squeeze\",\n",
        "    inputs=['input_tensor'],\n",
        "    outputs=['squeezed_tensor']\n",
        ")\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "squeeze_graph = make_graph([squeeze_node],\n",
        "                   'squeeze_model',\n",
        "                   [input_tensor],\n",
        "                   [squeezed_tensor])\n",
        "\n",
        "# Specify opset versions\n",
        "squeeze_model = make_model(\n",
        "    squeeze_graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('', 13) \n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(squeeze_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"squeezed_node.onnx\", \"wb\") as f:\n",
        "    f.write(squeeze_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rotDwkAmtU5Q",
        "outputId": "d55b0099-6a48-44e9-df73-3a3b6037ddac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([ 2,  2,  1,  3, -1], dtype=int64)]"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('squeezed_node.onnx')\n",
        "input_data = np.array([[ 2,  2,  1,  3, -1]], dtype=np.int64)\n",
        "outputs = ort_sess.run(None, {'input_tensor': input_data})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFqpYN4zuzKx"
      },
      "source": [
        "# Unsqueeze\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#Unsqueeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "L4L6oUcVrZqb"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "squeezed_tensor = make_tensor_value_info('squeezed_tensor', TensorProto.INT64, [None])\n",
        "\n",
        "# Shape tensor to specify the output shape\n",
        "unsqueeze_tensor = make_tensor_value_info('unsqueeze_tensor', TensorProto.INT64, [1, None])\n",
        "\n",
        "# Create axes tensor to specify the axes to unsqueeze\n",
        "axes_tensor = make_tensor_value_info('axes_tensor', TensorProto.INT64, [1])\n",
        "\n",
        "# Create Unsqueeze node\n",
        "unsqueeze_node = make_node(\n",
        "    'Unsqueeze',\n",
        "    inputs=['squeezed_tensor', 'axes_tensor'],\n",
        "    outputs=['unsqueeze_tensor'],\n",
        ")\n",
        "\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "unsqueeze_graph = make_graph([unsqueeze_node],\n",
        "                   'unsqueeze_model',\n",
        "                   [squeezed_tensor, axes_tensor],\n",
        "                   [unsqueeze_tensor])\n",
        "\n",
        "# Specify opset versions\n",
        "unsqueeze_model = make_model(\n",
        "    unsqueeze_graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('', 13)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(unsqueeze_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"unsqueeze_node.onnx\", \"wb\") as f:\n",
        "    f.write(unsqueeze_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIs1iKKBtzvC",
        "outputId": "56d215c2-3adc-4231-a443-facb74082dd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 2,  2,  1,  3, -1]], dtype=int64)]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('unsqueeze_node.onnx')\n",
        "input_data = np.array([ 2,  2,  1,  3, -1], dtype=np.int64)\n",
        "axes = np.array([0]).astype(np.int64)\n",
        "outputs = ort_sess.run(None, {'squeezed_tensor': input_data, 'axes_tensor': axes})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI6e1f82K1Zy"
      },
      "source": [
        "# Flatten\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "pLCLZJg5IYzP"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "input_tensor = make_tensor_value_info('input_tensor', TensorProto.INT64, [1, None])\n",
        "\n",
        "# Shape tensor to specify the output shape\n",
        "shape_tensor = make_tensor_value_info('shape_tensor', TensorProto.INT64, [1])\n",
        "\n",
        "# Constant node to provide the shape\n",
        "const_shape_node = make_node(\n",
        "    \"Constant\",\n",
        "    inputs=[],\n",
        "    outputs=[\"shape_tensor\"],\n",
        "    value=make_tensor(name=\"const_tensor\",\n",
        "                             data_type=TensorProto.INT64,\n",
        "                             dims=[1],\n",
        "                             vals=[-1])\n",
        ")\n",
        "\n",
        "# Reshape node to flatten the 2D array into a 1D array\n",
        "reshape_node = make_node(\n",
        "    \"Reshape\",\n",
        "    inputs=[\"input_tensor\", \"shape_tensor\"],\n",
        "    outputs=[\"output_tensor\"]\n",
        ")\n",
        "\n",
        "# Output tensor\n",
        "output_tensor = make_tensor_value_info('output_tensor', TensorProto.INT64, [None])\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "flatten_graph = make_graph([const_shape_node, reshape_node],\n",
        "                   'flatten_model',\n",
        "                   [input_tensor],\n",
        "                   [output_tensor])\n",
        "\n",
        "# Specify opset versions\n",
        "flatten_model = make_model(\n",
        "    flatten_graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('', 13)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(flatten_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"flatten_node.onnx\", \"wb\") as f:\n",
        "    f.write(flatten_model.SerializeToString())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItCQYyqgIaHX",
        "outputId": "47627cef-553e-4864-e099-b905f662663a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([ 2,  2,  1,  3, -1], dtype=int64)]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('flatten_node.onnx')\n",
        "input_data = np.array([[ 2,  2,  1,  3, -1]], dtype=np.int64)\n",
        "outputs = ort_sess.run(None, {'input_tensor': input_data})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZUt9xheyxVK"
      },
      "source": [
        "# Slice\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#Slice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReZMWXsRyztG",
        "outputId": "55d5db33-8659-47b1-b04c-1ad6c098a9dd"
      },
      "outputs": [],
      "source": [
        "# Define the ONNX node\n",
        "slice_node = make_node(\n",
        "    \"Slice\",\n",
        "    inputs=[\"x\", \"starts\", \"ends\"],\n",
        "    outputs=[\"y\"],\n",
        ")\n",
        "\n",
        "# Create the ONNX graph with the defined node\n",
        "slice_graph = make_graph(\n",
        "    [slice_node],\n",
        "    \"slice_model\",\n",
        "    inputs=[\n",
        "        make_tensor_value_info(\"x\", TensorProto.INT64, [1, None]),\n",
        "        make_tensor_value_info(\"starts\", TensorProto.INT64, [2]),\n",
        "        make_tensor_value_info(\"ends\", TensorProto.INT64, [2]),\n",
        "    ],\n",
        "    outputs=[\n",
        "        make_tensor_value_info(\"y\", TensorProto.INT64, [1, None]),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Create the ONNX model with the defined graph\n",
        "slice_model = make_model(\n",
        "    slice_graph,\n",
        "    opset_imports=[\n",
        "        make_opsetid('', 11)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(slice_model)\n",
        "\n",
        "\n",
        "# Save the split model to a file\n",
        "with open(\"slice_node.onnx\", \"wb\") as f:\n",
        "    f.write(slice_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: (1, 3)\n",
            "Output: [[1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('slice_node.onnx')\n",
        "\n",
        "# Create input data\n",
        "x = np.array([[1, 1, 0, -1, -1]], dtype=np.int64)\n",
        "starts = np.array([0, 0], dtype=np.int64)\n",
        "ends = np.array([1, 3], dtype=np.int64)\n",
        "\n",
        "# Run the inference\n",
        "output = ort_sess.run(None, {\"x\": x, \"starts\": starts, \"ends\": ends})\n",
        "\n",
        "# Output shape and values\n",
        "print(\"Output shape:\", output[0].shape)\n",
        "print(\"Output:\", output[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EkWFwEpm5zZ"
      },
      "source": [
        "# Padding\n",
        "https://github.com/onnx/onnx/blob/main/docs/Operators.md#Pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "K0eaFz8Cm5Cu"
      },
      "outputs": [],
      "source": [
        "# Define the ONNX node\n",
        "padd_node = make_node(\n",
        "    \"Pad\",\n",
        "    inputs=[\"x\", \"pads\"],\n",
        "    outputs=[\"y\"],\n",
        "    mode=\"constant\"\n",
        ")\n",
        "\n",
        "# Create ONNX graph\n",
        "padd_graph = make_graph(\n",
        "    [padd_node],\n",
        "    \"padd_model\",\n",
        "    inputs=[\n",
        "        make_tensor_value_info(\"x\", TensorProto.INT64, [1, None]),  # Input shape [None], allowing variable-length input\n",
        "        make_tensor_value_info(\"pads\", TensorProto.INT64, [4]),  # Pads shape [2] for a 1D input\n",
        "    ],\n",
        "    outputs=[make_tensor_value_info(\"y\", TensorProto.INT64, [1, None])],  # Output shape [10]\n",
        ")\n",
        "\n",
        "# Create ONNX model\n",
        "padd_model = make_model(\n",
        "    padd_graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('', 13) \n",
        "    ]\n",
        ")\n",
        "\n",
        "# Verify the model\n",
        "check_model(padd_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"padding_node.onnx\", \"wb\") as f:\n",
        "    f.write(padd_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoJ2RbKTw-om",
        "outputId": "7124196c-57fe-4462-d8f3-dea03f3767b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the ONNX model\n",
        "session = ort.InferenceSession(\"padding_node.onnx\")\n",
        "\n",
        "# Prepare input data\n",
        "input_data = np.array([[1, 2, 3, 4, 5]], dtype=np.int64) \n",
        "pads = np.array([0, 0, 0, 10], dtype=np.int64)\n",
        "\n",
        "# Run inference\n",
        "output = session.run([\"y\"], {\"x\": input_data, \"pads\": pads})\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uJhoeJTI7gq"
      },
      "source": [
        "# Compose\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Opset vesrion 20, IR 9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3qAp7teIlFw4"
      },
      "outputs": [],
      "source": [
        "# Input tensor\n",
        "string_input = make_tensor_value_info('string_input', TensorProto.STRING, [1])\n",
        "# Intermediate tensor after splitting\n",
        "string_split = make_tensor_value_info('string_split', TensorProto.STRING, [None])\n",
        "# Intermediate tensor after lowercasing\n",
        "string_normalized = make_tensor_value_info('string_normalized', TensorProto.STRING, [None])\n",
        "# Output tensor\n",
        "numeric_output = make_tensor_value_info('numeric_output', TensorProto.INT64, [1, None])\n",
        "\n",
        "# String Split node\n",
        "split_node = make_node(\n",
        "    'StringSplit',\n",
        "    inputs=['string_input'],\n",
        "    outputs=['string_split', 'unused'],\n",
        "    delimiter=\"\",\n",
        "    maxsplit=None,\n",
        ")\n",
        "\n",
        "# String Split node\n",
        "split_node = make_node(\n",
        "    op_type=\"Tokenizer\",\n",
        "    inputs=[\"string_input\"],\n",
        "    outputs=[\"string_split\"],\n",
        "    mark=0,  # Mark the beginning/end character\n",
        "    mincharnum=1,  # Minimum number of characters allowed\n",
        "    pad_value=\"\",  # Padding value\n",
        "    separators=[\" \"],  # List of separators (space)\n",
        "    domain=\"com.microsoft\"  # Specify the domain\n",
        ")\n",
        "\n",
        "# String Normalizer node\n",
        "normalizer_node = make_node(\n",
        "    \"StringNormalizer\",\n",
        "    inputs=[\"string_split\"],\n",
        "    outputs=[\"string_normalized\"],\n",
        "    case_change_action=\"LOWER\",\n",
        "    stopwords=[\"the\"]\n",
        ")\n",
        "# Category Mapper node\n",
        "mapper_node = make_node(\n",
        "    'CategoryMapper',\n",
        "    inputs=['string_normalized'],\n",
        "    outputs=['numeric_output'],\n",
        "    cats_strings=[\"do\", \"not\", \"some\"],  # Vocabulary for mapping\n",
        "    cats_int64s=[1, 2, 3],  # Integer mapping of vocabulary\n",
        "    domain='ai.onnx.ml'\n",
        ")\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "graph = make_graph([split_node, normalizer_node, mapper_node],\n",
        "                   'numericalizer',\n",
        "                   [string_input],\n",
        "                   [numeric_output])\n",
        "\n",
        "# Specify opset versions\n",
        "onnx_model = make_model(\n",
        "    graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('ai.onnx.ml', 1),\n",
        "      make_opsetid('com.microsoft', 1),\n",
        "      make_opsetid('', 20)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(onnx_model)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"test_numericalizer_IR9.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xp5EwupI-4p"
      },
      "outputs": [],
      "source": [
        "ort_sess = ort.InferenceSession('test_numericalizer_1.onnx')\n",
        "\n",
        "x = np.array([\"NOT not do SOME the oov\"])\n",
        "outputs = ort_sess.run(None, {'string_input': x})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Opset vesrion 17, IR 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BqNxx0lQuMV",
        "outputId": "1b59bedc-4bfe-4ce8-c61d-aaf7308639ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IR Version: 8\n"
          ]
        }
      ],
      "source": [
        "# Input string tensor:\n",
        "# [\"MY CLEAN QUERY oov stopword\"]\n",
        "string_input = make_tensor_value_info('string_input', TensorProto.STRING, [1])\n",
        "# Intermediate tensor after splitting:\n",
        "# [\"MY\", \"CLEAN\", \"QUERY\", \"oov\", \"stopword\"]\n",
        "string_split = make_tensor_value_info('string_split', TensorProto.STRING, [None])\n",
        "# Intermediate tensor after normalization:\n",
        "# [\"my\", \"clean\", \"query\", \"oov\"]\n",
        "string_normalized = make_tensor_value_info('string_normalized', TensorProto.STRING, [None])\n",
        "# Mapper tensor:\n",
        "# [[1, 2, 3, -1]]\n",
        "numeric_output = make_tensor_value_info('numeric_output', TensorProto.INT64, [1, None])\n",
        "# Flatten:\n",
        "# [1, 2, 3, -1]\n",
        "shape_tensor = make_tensor_value_info('shape_tensor', TensorProto.INT64, [1])\n",
        "numeric_output_flatten = make_tensor_value_info('numeric_output_flatten', TensorProto.INT64, [None])\n",
        "# Pad inputs:\n",
        "pads = make_tensor_value_info(\"pads\", TensorProto.INT64, [2]) # Pads shape [2] for a 1D input\n",
        "# Pad output:\n",
        "# [1, 2, 3, -1, 0, 0, 0] (pad = 3)\n",
        "numeric_output_padded = make_tensor_value_info(\"numeric_output_padded\", TensorProto.INT64, [None])\n",
        "# Slice inputs:\n",
        "slice_start = make_tensor_value_info(\"slice_start\", TensorProto.INT64, [1])\n",
        "slice_end = make_tensor_value_info(\"slice_end\", TensorProto.INT64, [1])\n",
        "# Slice output:\n",
        "# [1, 2, 3] (start=0, end=3)\n",
        "numeric_output_sliced = make_tensor_value_info(\"numeric_output_sliced\", TensorProto.INT64, [None])\n",
        "\n",
        "# String Split node\n",
        "split_node = make_node(\n",
        "    op_type=\"Tokenizer\",\n",
        "    inputs=[\"string_input\"],\n",
        "    outputs=[\"string_split\"],\n",
        "    mark=0,  # Mark the beginning/end character\n",
        "    mincharnum=1,  # Minimum number of characters allowed\n",
        "    pad_value=\"\",  # Padding value\n",
        "    separators=[\" \"],  # List of separators (space)\n",
        "    domain=\"com.microsoft\"  # Specify the domain\n",
        ")\n",
        "# String Normalizer node\n",
        "normalizer_node = make_node(\n",
        "    \"StringNormalizer\",\n",
        "    inputs=[\"string_split\"],\n",
        "    outputs=[\"string_normalized\"],\n",
        "    case_change_action=\"LOWER\",\n",
        "    stopwords=[\"the\"] # HERE WE DEFINE THE STOP WORDS\n",
        ")\n",
        "# Category Mapper node\n",
        "mapper_node = make_node(\n",
        "    'CategoryMapper',\n",
        "    inputs=['string_normalized'],\n",
        "    outputs=['numeric_output'],\n",
        "    cats_strings=[\"do\", \"not\", \"some\"], # VOCAB FOR MAPPING\n",
        "    cats_int64s=[1, 2, 3], # INTEGER MAPPING OF VOCAB\n",
        "    default_int64=0, # Default oov token\n",
        "    domain='ai.onnx.ml'\n",
        ")\n",
        "# Constant Shape node\n",
        "const_shape_node = make_node(\n",
        "    \"Constant\",\n",
        "    inputs=[],\n",
        "    outputs=[\"shape_tensor\"],\n",
        "    value=make_tensor(name=\"const_tensor\",\n",
        "                             data_type=TensorProto.INT64,\n",
        "                             dims=[1],\n",
        "                             vals=[-1])\n",
        ")\n",
        "# Reshape node: flatten the 2D array into a 1D array\n",
        "reshape_node = make_node(\n",
        "    \"Reshape\",\n",
        "    inputs=[\"numeric_output\", \"shape_tensor\"],\n",
        "    outputs=[\"numeric_output_flatten\"]\n",
        ")\n",
        "# Padding node\n",
        "padding_node = make_node(\n",
        "    \"Pad\",\n",
        "    inputs=[\"numeric_output_flatten\", \"pads\"],\n",
        "    outputs=[\"numeric_output_padded\"],\n",
        "    mode=\"constant\"\n",
        ")\n",
        "# Slice node\n",
        "slice_node = make_node(\n",
        "    \"Slice\",\n",
        "    inputs=[\"numeric_output_padded\", \"slice_start\", \"slice_end\"],\n",
        "    outputs=[\"numeric_output_sliced\"],\n",
        ")\n",
        "# Create the graph with the new nodes\n",
        "graph = make_graph([split_node, normalizer_node, mapper_node, const_shape_node, reshape_node, padding_node, slice_node],\n",
        "                   'numericalizer',\n",
        "                   inputs=[string_input, pads, slice_start, slice_end],\n",
        "                   outputs=[numeric_output_sliced])\n",
        "\n",
        "# Specify opset versions\n",
        "onnx_model = make_model(\n",
        "    graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('ai.onnx.ml', 1),\n",
        "      make_opsetid('com.microsoft', 1),\n",
        "      make_opsetid('', 17)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(onnx_model)\n",
        "\n",
        "# Check the IR version\n",
        "ir_version = onnx_model.ir_version\n",
        "print(\"IR Version:\", ir_version)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"test_numericalizer_IR8.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "I3baYrCrKwfe",
        "fxqcUiE3K5OW",
        "wWTL-S8MOQY3",
        "9TQKxE65u6XT",
        "mFqpYN4zuzKx",
        "tI6e1f82K1Zy",
        "yZUt9xheyxVK",
        "5EkWFwEpm5zZ",
        "CPoIJgFevuCh",
        "7uJhoeJTI7gq",
        "hppU4bYnFlVt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
