{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zd8YhHCgmaE5"
      },
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "from onnx import TensorProto, save, load\n",
        "from onnx.helper import (\n",
        "    make_model, make_node, make_tensor, make_graph,\n",
        "    make_tensor_value_info, make_opsetid)\n",
        "from onnx.checker import check_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEIEN0Vv_fSv"
      },
      "source": [
        "# Numericalizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing parameters\n",
        "SEPARATORS=[\" \"]\n",
        "ACTION=\"LOWER\"\n",
        "STOPWORDS=[\"the\"]\n",
        "\n",
        "# Model parameters\n",
        "OUTPUT_LENGTH = 15\n",
        "VOCAB = [\"do\", \"not\", \"some\"]\n",
        "INT_MAP = [i+1 for i in range(len(VOCAB))]\n",
        "OOV_INT = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IR Version: 8\n",
            "OpsSet Version: 17\n"
          ]
        }
      ],
      "source": [
        "# Input string tensor:\n",
        "# [\"MY CLEAN QUERY oov stopword\"]\n",
        "string_input = make_tensor_value_info('string_input', TensorProto.STRING, [1])\n",
        "# [[\"MY\", \"CLEAN\", \"QUERY\", \"oov\", \"stopword\"]]\n",
        "string_split = make_tensor_value_info('string_split', TensorProto.STRING, [1, None])\n",
        "\n",
        "# Intermediate tensor after normalization:\n",
        "# [[\"my\", \"clean\", \"query\", \"oov\"]]\n",
        "string_normalized = make_tensor_value_info('string_normalized', TensorProto.STRING, [1, None])\n",
        "\n",
        "# Output tensor\n",
        "numeric_output = make_tensor_value_info('numeric_output', TensorProto.INT64, [1, None])\n",
        "\n",
        "# Pad inputs:\n",
        "pads_data = make_tensor(\"pads_data\", TensorProto.INT64, [4], [0, 0, 0, OUTPUT_LENGTH])  # Constant value of [0, OUTPUT_LENGTH]\n",
        "# Pad output:\n",
        "# [[1, 2, 3, -1, 0, 0, 0]] (pad = 3)\n",
        "numeric_output_padded = make_tensor_value_info(\"numeric_output_padded\", TensorProto.INT64, [1, None])\n",
        "\n",
        "# Slice inputs:\n",
        "slice_start_data = make_tensor(\"slice_start_data\", TensorProto.INT64, [2], [0, 0])  # Constant value of 0\n",
        "slice_end_data = make_tensor(\"slice_end_data\", TensorProto.INT64, [2], [1, OUTPUT_LENGTH])  # Constant value of OUTPUT_LENGTH\n",
        "# Slice output:\n",
        "# [[1, 2, 3]] (start=0, end=3)\n",
        "numeric_output_sliced = make_tensor_value_info(\"numeric_output_sliced\", TensorProto.INT64, [1, OUTPUT_LENGTH])\n",
        "\n",
        "# String Split node\n",
        "split_node = make_node(\n",
        "    op_type=\"Tokenizer\",\n",
        "    inputs=[\"string_input\"],\n",
        "    outputs=[\"string_split\"],\n",
        "    mark=0,  # Mark the beginning/end character\n",
        "    mincharnum=1,  # Minimum number of characters allowed\n",
        "    pad_value=\"\",  # Padding value\n",
        "    separators=SEPARATORS,  # List of separators (space)\n",
        "    domain=\"com.microsoft\"\n",
        ")\n",
        "\n",
        "# String Normalizer node\n",
        "normalizer_node = make_node(\n",
        "    \"StringNormalizer\",\n",
        "    inputs=[\"string_split\"],\n",
        "    outputs=[\"string_normalized\"],\n",
        "    case_change_action=ACTION,\n",
        "    stopwords=STOPWORDS # HERE WE DEFINE THE STOP WORDS\n",
        ")\n",
        "\n",
        "# CategoryMapper node\n",
        "mapper_node = make_node(\n",
        "    'CategoryMapper',\n",
        "    ['string_normalized'],\n",
        "    ['numeric_output'],\n",
        "    cats_strings=[\"do\", \"not\"],  # Vocabulary for mapping\n",
        "    cats_int64s=[1, 2],  # Integer mapping of\n",
        "    default_int64=0, # Default oov token\n",
        "    domain='ai.onnx.ml'\n",
        ")\n",
        "\n",
        "# Constant node for pads\n",
        "pads_constant_node = make_node(\n",
        "    \"Constant\",\n",
        "    inputs=[],\n",
        "    outputs=[\"pads\"],\n",
        "    value=pads_data\n",
        ")\n",
        "# Padding node\n",
        "padding_node = make_node(\n",
        "    \"Pad\",\n",
        "    inputs=[\"numeric_output\", \"pads\"],\n",
        "    outputs=[\"numeric_output_padded\"],\n",
        "    mode=\"constant\"\n",
        ")\n",
        "\n",
        "# Constant for slice_start\n",
        "slice_start_constant_node = make_node(\n",
        "    \"Constant\",\n",
        "    inputs=[],\n",
        "    outputs=[\"slice_start\"],\n",
        "    value=slice_start_data\n",
        ")\n",
        "# Constant for slice_end\n",
        "slice_end_constant_node = make_node(\n",
        "    \"Constant\",\n",
        "    inputs=[],\n",
        "    outputs=[\"slice_end\"],\n",
        "    value=slice_end_data\n",
        ")\n",
        "# Slice node\n",
        "slice_node = make_node(\n",
        "    \"Slice\",\n",
        "    inputs=[\"numeric_output_padded\", \"slice_start\", \"slice_end\"],\n",
        "    outputs=[\"numeric_output_sliced\"],\n",
        ")\n",
        "\n",
        "# Create the graph with the new nodes\n",
        "graph = make_graph(\n",
        "    [\n",
        "      split_node,\n",
        "      normalizer_node,\n",
        "      mapper_node,\n",
        "      pads_constant_node,\n",
        "      padding_node,\n",
        "      slice_start_constant_node,\n",
        "      slice_end_constant_node,\n",
        "      slice_node,\n",
        "    ],\n",
        "    'numericalizer',\n",
        "    inputs=[string_input],\n",
        "    outputs=[numeric_output_sliced]\n",
        ")\n",
        "\n",
        "# Specify opset versions\n",
        "onnx_model = make_model(\n",
        "    graph,\n",
        "    opset_imports=[\n",
        "      make_opsetid('ai.onnx.ml', 1),\n",
        "      make_opsetid('com.microsoft', 1),\n",
        "      make_opsetid('', 17)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Check the model consistency\n",
        "check_model(onnx_model)\n",
        "\n",
        "# Check the IR version\n",
        "ir_version = onnx_model.ir_version\n",
        "print(\"IR Version:\", ir_version)\n",
        "\n",
        "# Check the OpsSet version\n",
        "ops_set_version = onnx_model.opset_import[2].version \n",
        "print(\"OpsSet Version:\", ops_set_version)\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"tokenizer.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)]\n",
            "[array([[2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)]\n",
            "[array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], dtype=int64)]\n",
            "[array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)]\n"
          ]
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('tokenizer.onnx')\n",
        "\n",
        "x = np.array([\"NOT not do SOME the oov\"])\n",
        "print(ort_sess.run(None, {'string_input': x}))\n",
        "\n",
        "x = np.array([\"NOT not do SOME the oov, and this is a longer string with more than 15 chars\"])\n",
        "print(ort_sess.run(None, {'string_input': x}))\n",
        "\n",
        "x = np.array([\"NOT not NOT not NOT not NOT not NOT not NOT not NOT not NOT\"])\n",
        "print(ort_sess.run(None, {'string_input': x}))\n",
        "\n",
        "x = np.array([\"NOT\"])\n",
        "print(ort_sess.run(None, {'string_input': x}))\n",
        "\n",
        "outputs = ort_sess.run(None, {'string_input': x})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCxxcpr5ItV9"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rz2fsIfCh7-A"
      },
      "outputs": [],
      "source": [
        "numeric_vector=outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3uIiPM638qN",
        "outputId": "a71e70a7-8704-4dd8-be36-6d56e6a5adfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "Inference Output:\n",
            "tensor([[ 0.9169,  1.1620, -1.0315,  0.0393,  1.0036, -0.0645, -1.3321, -0.6720,\n",
            "          0.4619, -2.3518,  1.4597,  1.4690, -0.0885,  0.3078,  0.1892,  0.9687,\n",
            "          1.0791,  1.4366, -0.4015,  0.5671,  0.1021, -0.2049, -0.8865, -0.6974,\n",
            "         -0.2545, -0.3683,  0.2314, -0.0735, -1.0602,  0.1250,  0.2749, -0.4889,\n",
            "         -0.6295, -0.2636,  0.5858, -0.0387, -2.1346, -1.4208,  0.2145, -0.3342,\n",
            "         -0.3430, -0.6142, -0.2990,  0.0913, -0.0588, -1.1512,  1.4431,  0.6062,\n",
            "         -0.5437,  0.2913,  1.8397, -1.1921, -0.2544, -0.7286, -0.3283, -1.0158,\n",
            "          0.2401, -0.5097,  0.8891, -1.6871,  0.8605, -0.0974,  1.0748, -1.6056,\n",
            "         -2.0465,  0.7030,  0.1599,  0.7558,  0.9531, -1.1229,  1.2865,  0.2936,\n",
            "          0.0306,  1.5168,  0.3206,  0.9903, -2.4394,  1.3872,  1.0919,  0.7916,\n",
            "         -0.7218,  1.3455,  0.5363, -0.9169, -0.2500,  0.8704,  0.0632,  0.9085,\n",
            "         -1.3063, -0.4256, -0.2826,  0.7079, -0.5164,  0.1166, -0.1054,  0.6139,\n",
            "          0.7784,  2.0629, -0.3540,  0.1149]], grad_fn=<MeanBackward1>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(EmbeddingModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        pooled = torch.mean(embedded, dim=1)  # Global average pooling\n",
        "        return pooled\n",
        "\n",
        "# Example vocabulary\n",
        "vocab_size = 10000  # Adjust this according to your vocabulary size\n",
        "embedding_dim = 100  # Length of the embedding vector\n",
        "\n",
        "# Initialize the model\n",
        "model = EmbeddingModel(vocab_size, embedding_dim)\n",
        "\n",
        "# Load the numeric vector into a PyTorch tensor\n",
        "input_tensor = torch.tensor(outputs[0])\n",
        "print(input_tensor)\n",
        "\n",
        "# Perform inference\n",
        "output = model(input_tensor)\n",
        "print(\"Inference Output:\")\n",
        "print(output)\n",
        "\n",
        "# Export the model to ONNX format\n",
        "torch.onnx.export(model,\n",
        "                  input_tensor,\n",
        "                  \"embedding_model.onnx\",\n",
        "                  input_names=[\"numeric_input_tensor\"],\n",
        "                  output_names=[\"output_embedding\"],\n",
        "                  opset_version=17)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpqfKKNWDNDw",
        "outputId": "c318d4eb-96a5-497d-d6f7-8b7ef0cb9995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IR Version: 8\n",
            "OpsSet Version: 17\n"
          ]
        }
      ],
      "source": [
        "# Check the IR version\n",
        "model = load(\"embedding_model.onnx\")\n",
        "ir_version = model.ir_version\n",
        "print(\"IR Version:\", ir_version)\n",
        "\n",
        "# Check the OpsSet version\n",
        "ops_set_version = model.opset_import[0].version if model.opset_import else None\n",
        "print(\"OpsSet Version:\", ops_set_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFH0rXZz7muZ",
        "outputId": "5cf94b7a-b43b-4e94-b1e3-a5029336fdfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 0.7538846 ,  0.6315539 , -0.858148  , -0.31353578,  0.61638606,\n",
              "          0.03712944, -0.95248246, -0.6610899 ,  0.44543356, -1.9777925 ,\n",
              "          1.114134  ,  1.0663089 , -0.0475119 ,  0.3085864 ,  0.05627583,\n",
              "          0.8581572 ,  0.8341553 ,  1.1332772 , -0.35968867,  0.4783103 ,\n",
              "          0.1064309 , -0.21061324, -0.6963769 , -0.55464876, -0.36087307,\n",
              "         -0.29060012,  0.03030721, -0.17939591, -0.8041696 , -0.01990776,\n",
              "          0.2336085 , -0.4513607 , -0.462214  , -0.04942515,  0.6072999 ,\n",
              "         -0.02932008, -1.7095354 , -1.0576055 ,  0.26565388, -0.44676393,\n",
              "         -0.4330632 , -0.6291286 , -0.18799762,  0.0549604 , -0.24652083,\n",
              "         -1.0596744 ,  1.0244541 ,  0.36720684, -0.57796216,  0.10362174,\n",
              "          1.557532  , -0.99402636, -0.04505811, -0.8164367 , -0.19240957,\n",
              "         -0.7551031 , -0.02828829, -0.39015922,  0.7850735 , -1.3096771 ,\n",
              "          0.62475246, -0.23281808,  0.9300837 , -1.4388709 , -1.731892  ,\n",
              "          0.49255267,  0.24945937,  0.6274666 ,  0.7214422 , -1.045097  ,\n",
              "          1.0506536 ,  0.42453828, -0.08760943,  1.2849863 ,  0.30923378,\n",
              "          0.89853483, -1.9927089 ,  1.0170118 ,  0.952234  ,  0.6395827 ,\n",
              "         -0.7914103 ,  1.1947805 ,  0.5356854 , -0.591462  , -0.38689902,\n",
              "          0.7204514 ,  0.30943063,  0.6763731 , -1.0390513 , -0.5680619 ,\n",
              "         -0.24151948,  0.5311357 , -0.49036705,  0.11788872, -0.20007578,\n",
              "          0.64166677,  0.7341059 ,  1.6549435 , -0.18332201,  0.01978688]],\n",
              "       dtype=float32)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('embedding_model.onnx')\n",
        "\n",
        "# String input\n",
        "x = np.array([[2, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
        "\n",
        "outputs = ort_sess.run(None, {'numeric_input_tensor': x})\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hppU4bYnFlVt"
      },
      "source": [
        "# Version Converter\n",
        "https://github.com/onnx/tutorials/blob/main/tutorials/VersionConversion.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hYyS-qhTE4IJ"
      },
      "outputs": [],
      "source": [
        "# from onnx import version_converter\n",
        "# # Load the model\n",
        "# model = load(\"embedding_model.onnx\")\n",
        "\n",
        "# # Check that the IR is well formed\n",
        "# check_model(model)\n",
        "\n",
        "# # Convert to version 8\n",
        "# converted_model = version_converter.convert_version(onnx_model, 9)\n",
        "\n",
        "# # Save model\n",
        "# save(converted_model, \"embedding_model_v8.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nLyJdQSI26W"
      },
      "source": [
        "# ONNX Compose\n",
        "https://onnx.ai/onnx/api/compose.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "asYBYrFzI5e1"
      },
      "outputs": [],
      "source": [
        "from onnx.compose import merge_models\n",
        "\n",
        "# Load the two ONNX models\n",
        "model1 = load(\"tokenizer.onnx\")\n",
        "model2 = load(\"embedding_model.onnx\")\n",
        "\n",
        "# Merge the models\n",
        "io_map = [(\"numeric_output_sliced\", \"numeric_input_tensor\")]\n",
        "merged_model = merge_models(m1=model1, m2=model2, io_map=io_map)\n",
        "\n",
        "# Save the merged model\n",
        "save(merged_model, \"encoder.onnx\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7K-50tGhQMT",
        "outputId": "2a43c5f6-1d08-4170-def2-a0e4584688d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([[ 0.80201274,  0.82813585, -0.87482804, -0.22561213,  0.8256478 ,\n",
              "          0.01852857, -0.9869802 , -0.70129794,  0.41962573, -2.1136203 ,\n",
              "          1.2512188 ,  1.280487  , -0.07217049,  0.39154813,  0.10505456,\n",
              "          0.82849044,  0.91709095,  1.2231282 , -0.35748798,  0.4764018 ,\n",
              "          0.05872453, -0.24836732, -0.7213394 , -0.56056064, -0.3087985 ,\n",
              "         -0.3926673 ,  0.1158082 , -0.06045493, -0.8403996 , -0.04947696,\n",
              "          0.27079332, -0.3402849 , -0.54239565, -0.10022059,  0.5104051 ,\n",
              "         -0.03707116, -1.9227523 , -1.2315072 ,  0.25324383, -0.30595896,\n",
              "         -0.44820789, -0.6083864 , -0.1130712 ,  0.0343467 , -0.15431458,\n",
              "         -1.062029  ,  1.1480559 ,  0.4773775 , -0.6061598 ,  0.07696819,\n",
              "          1.6609509 , -1.0828117 , -0.06846457, -0.9380718 , -0.19315866,\n",
              "         -0.89049876,  0.02822874, -0.4400327 ,  0.7650407 , -1.3940212 ,\n",
              "          0.7116711 , -0.29284608,  1.0068922 , -1.528401  , -1.8572272 ,\n",
              "          0.5608603 ,  0.1534393 ,  0.68534434,  0.7898914 , -1.0199833 ,\n",
              "          1.0260141 ,  0.4552721 , -0.00519405,  1.3519903 ,  0.28805473,\n",
              "          0.851923  , -2.105157  ,  1.1389068 ,  0.98057747,  0.7337062 ,\n",
              "         -0.73836   ,  1.292102  ,  0.5593397 , -0.6757299 , -0.32345968,\n",
              "          0.8065886 ,  0.22518466,  0.6768195 , -1.0826477 , -0.5354073 ,\n",
              "         -0.20656428,  0.6436731 , -0.4983674 ,  0.15601069, -0.15933299,\n",
              "          0.62278664,  0.6712767 ,  1.7454797 , -0.30914813,  0.11088529]],\n",
              "       dtype=float32)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ort_sess = ort.InferenceSession('encoder.onnx')\n",
        "\n",
        "# String input\n",
        "x = np.array([\"NOT not do SOME the oov\"])\n",
        "\n",
        "outputs = ort_sess.run(None, {'string_input': x})\n",
        "outputs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "I3baYrCrKwfe",
        "fxqcUiE3K5OW",
        "wWTL-S8MOQY3",
        "9TQKxE65u6XT",
        "mFqpYN4zuzKx",
        "tI6e1f82K1Zy",
        "yZUt9xheyxVK",
        "5EkWFwEpm5zZ",
        "CPoIJgFevuCh",
        "7uJhoeJTI7gq",
        "hppU4bYnFlVt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
